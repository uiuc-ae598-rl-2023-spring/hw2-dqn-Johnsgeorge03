import torchimport torch.nn as nnimport torch.nn.functional as Fimport torch.optim as optimimport numpy as npclass DQN(nn.Module):        def __init__(self, n_observation, n_actions, hidden_size):        super(DQN, self).__init__()        self.layer1 = nn.Linear(n_observation, hidden_size)        self.layer2 = nn.Linear(hidden_size, hidden_size)        self.layer3 = nn.Linear(hidden_size, n_actions)                self.device      = ""        if torch.backends.mps.is_available():            self.device  = "mps"        elif torch.cuda.is_available():            self.device  = "cuda"        else:            self.device  = "cpu"                    self.to(self.device)        def forward(self, x):        x = F.tanh(self.layer1(x))        x = F.tanh(self.layer2(x))        return self.layer3(x)    class Agent:    def __init__(self, n_observations, n_actions, hidden_size,                  learning_rate, gamma, epsilon_start, epsilon_end,                  epsilon_decay):            self.n_observations = n_observations        self.n_actions      = n_actions        self.hidden_size    = hidden_size        self.learning_rate  = learning_rate        self.gamma          = gamma        self.epsilon_start  = epsilon_start        self.epsilon_end    = epsilon_end        self.epsilon_decay  = epsilon_decay        self.epsilon        = epsilon_start        self.memory         = []        self.memory_size    = 10000        self.batch_size     = 32        self.update_freq    = 1000        self.steps          = 0        self.anneal_steps   = 1e6                        self.q_network      = DQN(n_observations, n_actions, hidden_size)        self.target_network = DQN(n_observations, n_actions, hidden_size)        self.target_network.load_state_dict(self.q_network.state_dict())        self.target_network.eval()                        self.loss_fn        = nn.MSELoss()        self.optimizer      = optim.AdamW(self.q_network.parameters(),                                          lr = learning_rate, amsgrad=True)        # self.optimizer      = optim.RMSprop(self.q_network.parameters(),        #                                     lr = learning_rate, alpha = 0.95,        #                                     eps = 0.01, momentum = 0.95)                   def choose_action(self, state):        if np.random.rand() < self.epsilon:            return np.random.choice(self.n_actions)        else:            with torch.no_grad():                state  = torch.FloatTensor(state).unsqueeze(0).to(self.q_network.device)                q_vals = self.q_network(state)                return int(torch.where(q_vals == q_vals.max())[0][0])                #return q_vals.argmax().item()            def store_transition(self, state, action, reward, next_state, done):        experience = (state, action, reward, next_state, done)        self.memory.append(experience)        if len(self.memory) > self.memory_size:            del self.memory[0]                def replay_and_learn(self):        if len(self.memory) < self.batch_size:            return         batch = np.random.choice(len(self.memory), self.batch_size, replace=False)        state_batch = torch.FloatTensor([self.memory[i][0] for i in batch]).to(self.q_network.device)        action_batch = torch.LongTensor([self.memory[i][1] for i in batch]).to(self.q_network.device)        reward_batch = torch.FloatTensor([self.memory[i][2] for i in batch]).to(self.q_network.device)        next_state_batch = torch.FloatTensor([self.memory[i][3] for i in batch]).to(self.q_network.device)        done_batch = torch.FloatTensor([self.memory[i][4] for i in batch]).to(self.q_network.device)                        q_values = self.q_network(state_batch)        next_q_values = self.target_network(next_state_batch).detach()        target_q_values = reward_batch + (1 - done_batch) * self.gamma * next_q_values.max(1)[0]        q_values = q_values.gather(1, action_batch.unsqueeze(1)).squeeze(1)                # print("q_val shape: ", q_values.shape)        # print("tar_val: ", target_q_values.shape)        # td_error = target_q_values - q_values        # clipped_td_error = torch.clamp(td_error, -1, 1)        # loss = (clipped_td_error ** 2).mean()                criterion = nn.SmoothL1Loss()        loss  = criterion(q_values.unsqueeze(1), target_q_values.unsqueeze(1)).to(self.q_network.device)        # loss = self.loss_fn(q_values, target_q_values.unsqueeze(1)).to(self.q_network.device)                self.optimizer.zero_grad()        loss.backward()        # clip gradient norm        torch.nn.utils.clip_grad_value_(self.q_network.parameters(), 100.0)        self.optimizer.step()                self.steps += 1        if self.steps % self.update_freq == 0:            self.target_network.load_state_dict(self.q_network.state_dict())                    self.epsilon = max(self.epsilon_end, self.epsilon_start - (self.steps / self.anneal_steps)                            * (self.epsilon_start - self.epsilon_end))                                            