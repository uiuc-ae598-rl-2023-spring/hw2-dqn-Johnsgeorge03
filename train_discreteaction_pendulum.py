from agent_network import *import matplotlib.pyplot as pltimport discreteaction_pendulumimport seaborn as snsimport pandas as pd# def main():## ENVIRONMENTenv                 = discreteaction_pendulum.Pendulum()n_observations      = env.num_statesn_actions           = env.num_actions## HYPERPARAMETERShidden_size         = 128gamma               = 0.95learning_rate       = 0.0001epsilon_start       = 1.0epsilon_end         = 0.1epsilon_decay       = 0.999995## AGENTagent               = Agent(n_observations, n_actions, hidden_size,                            learning_rate, gamma, epsilon_start,                             epsilon_end, epsilon_decay)agent.batch_size    = 128agent.memory_size   = 100000agent.update_freq   = 100agent.anneal_steps  = 2e4## TRAINnum_episodes        = 10000scores              = []mean_scores         = []max_mean            = 0all_disc_r          = []all_udissc_r        = []for i in range(num_episodes):    s               = env.reset()    done            = False    total_reward    = 0    rewards_list    = []    disc_r_list     = []    count           = 0    while not done:        action      = agent.choose_action(s)         assert(action in np.arange(n_actions))        s1, r, done = env.step(action)        agent.store_transition(s, action, r, s1, done)        agent.replay_and_learn()        disc_r_list.append((gamma**count)*r)        rewards_list.append(r)        total_reward += r        s           = s1        count += 1            all_disc_r.append(disc_r_list)    all_udissc_r.append(rewards_list)    scores.append(total_reward)    mean = np.mean(scores)    mean_scores.append(mean)    if mean > max_mean:        torch.save(agent.q_network.state_dict(), 'qnet_model_weights_max_mean.pth')        torch.save(agent.target_network.state_dict(), 'tnet_model_weights_max_mean.pth')        max_mean = mean            print('Episode {}, Total iteraitons {}, Total Reward {:.2f}, Mean Reward {:.2f}, Epsilon: {:.2f}'          .format(i + 1, agent.steps,            total_reward, np.mean(scores[-100:]), agent.epsilon))torch.save(agent.q_network.state_dict(), 'qnet_model_weights_end.pth')torch.save(agent.target_network.state_dict(), 'tnet_model_weights_end.pth')plt.figure()plt.plot(mean_scores)plt.legend(loc = 'best')plt.xlabel("Episodes")plt.ylabel("Mean episodic returns")plt.title("DQN learning curve")plt.grid()plt.show()        # calculate the mean and standard deviation of each rowarr = np.array(all_disc_r)row_means = np.mean(arr, axis=1)row_stds = np.std(arr, axis=1)# create a pandas DataFrame with columns for x-axis, y-axis, lower bound, and upper bounddf = pd.DataFrame({'x': range(arr.shape[0]),                   'y': row_means,                   'lower': row_means - row_stds,                   'upper': row_means + row_stds})# plot the mean values with a variance band using Seaborn's lineplotsns.lineplot(data=df, x='x', y='y', ci='sd')# plot the variance band as a shaded areaplt.fill_between(df['x'], df['lower'], df['upper'], alpha=0.2)# set the plot labels and legendplt.ylim(bottom=np.min(row_means - row_stds), top=np.max(row_means + 1.5*row_stds))plt.xlabel('Episodes')plt.ylabel('Mean Discounted Rewards')plt.title("DQN learning curve")# show the plotplt.show()arr = np.array(all_udissc_r)row_means = np.mean(arr, axis=1)row_stds = np.std(arr, axis=1)# create a pandas DataFrame with columns for x-axis, y-axis, lower bound, and upper bounddf = pd.DataFrame({'x': range(arr.shape[0]),                   'y': row_means,                   'lower': row_means - row_stds,                   'upper': row_means + row_stds})# plot the mean values with a variance band using Seaborn's lineplotsns.lineplot(data=df, x='x', y='y', ci='sd')# plot the variance band as a shaded areaplt.fill_between(df['x'], df['lower'], df['upper'], alpha=0.2)# set the plot labels and legendplt.ylim(bottom=np.min(row_means - row_stds), top=np.max(row_means + 1.5*row_stds))plt.xlabel('Episodes')plt.ylabel('Mean Undiscounted Rewards')plt.title("DQN learning curve")# show the plotplt.show()# if __name__ == '__main__':#     main()